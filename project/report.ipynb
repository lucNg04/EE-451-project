{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f23e1c5",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Final project - Chocolate Recognition\n",
    "\n",
    "\n",
    "**Moodle group ID:** *xx*  \n",
    "**Kaggle challenge:** *xx* (either `Classic` or `Deep learning`)  \n",
    "**Kaggle team name (exact):** \"*xx*\"  \n",
    "\n",
    "**Author 1 (SCIPER):** *Student Name 1 (xxxxx)*  \n",
    "**Author 2 (SCIPER):** *Student Name 2 (xxxxx)*  \n",
    "**Author 3 (SCIPER):** *Student Name 3 (xxxxx)*  \n",
    "\n",
    "**Due date:** 21.05.2025 (11:59 pm)\n",
    "\n",
    "\n",
    "## Key Submission Guidelines:\n",
    "- **Before submitting your notebook, <span style=\"color:red;\">rerun</span> it from scratch!** Go to: `Kernel` > `Restart & Run All`\n",
    "- **Only groups of three will be accepted**, except in exceptional circumstances.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84a38df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# src/classification_dataset.py\n",
    "# ==============================\n",
    "\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    # def __init__(self, img_dir, csv_path, transform=None):\n",
    "    #     self.img_dir = img_dir\n",
    "    #     self.data = pd.read_csv(csv_path)\n",
    "    #     self.transform = transform\n",
    "\n",
    "    # def __len__(self):\n",
    "    #     return len(self.data)\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row = self.data.iloc[idx]\n",
    "    #     img_path = os.path.join(self.img_dir, row['filename'])\n",
    "    #     image = Image.open(img_path).convert(\"RGB\")\n",
    "    #     labels = torch.tensor(row[1:].values.astype(float), dtype=torch.float32)\n",
    "\n",
    "    #     if self.transform:\n",
    "    #         image = self.transform(image)\n",
    "\n",
    "    #     return image, labels\n",
    "        def __init__(self, csv_file, img_dir, transform=None):\n",
    "            self.labels_df = pd.read_csv(csv_file)\n",
    "            self.img_dir = img_dir\n",
    "            self.transform = transform or T.Compose([\n",
    "                T.Resize((224, 224)),\n",
    "                T.ToTensor()\n",
    "                ])\n",
    "            self.image_ids = self.labels_df['id'].astype(str)\n",
    "            self.labels = self.labels_df.drop(columns=['id']).values.astype(float)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.image_ids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_id = self.image_ids.iloc[idx]\n",
    "            img_path = os.path.join(self.img_dir, f\"L{img_id}.JPG\")\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            image = self.transform(image)\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ee8fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# src/classification_model.py\n",
    "# ==============================\n",
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "\n",
    "def get_classification_model(num_labels):\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, num_labels),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "004cf3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# src/detection_dataset.py\n",
    "# ==============================\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class DetectionDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transforms = transforms\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith(('.JPG', '.png'))]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        label_path = os.path.join(self.label_dir, img_name.replace('.JPG', '.txt').replace('.png', '.txt'))\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = image.size\n",
    "        boxes, labels = [], []\n",
    "\n",
    "        with open(label_path) as f:\n",
    "            for line in f:\n",
    "                class_id, x, y, w_ratio, h_ratio = map(float, line.strip().split())\n",
    "                x1 = (x - w_ratio/2) * w\n",
    "                y1 = (y - h_ratio/2) * h\n",
    "                x2 = (x + w_ratio/2) * w\n",
    "                y2 = (y + h_ratio/2) * h\n",
    "                boxes.append([x1, y1, x2, y2])\n",
    "                labels.append(int(class_id))\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe069a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# src/detection_model.py\n",
    "# ==============================\n",
    "import torchvision\n",
    "\n",
    "def get_detection_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a7c27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# src/train_classification.py\n",
    "# ==============================\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_classification():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    dataset = ClassificationDataset(\"dataset_project_iapr2025/train\", \"dataset_project_iapr2025/train.csv\", transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = get_classification_model(num_labels=13)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} done\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"weights/classification_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ba6986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "def train_detection():\n",
    "    dataset = DetectionDataset(\"D:/exchange/EE-451/Choco/EE-451-project/project/dataset_project_iapr2025/train\", \"D:/exchange/EE-451/Choco/EE-451-project/project/choco_annotation/obj_train_data\", transforms=ToTensor())\n",
    "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "    model = get_detection_model(num_classes=13)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"gpu running\")\n",
    "    else:\n",
    "        print(\"No gpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} done\")\n",
    "\n",
    "    torch.save(model.state_dict(), \"weights/detection_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf2494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    model_det = get_detection_model(num_classes=5)\n",
    "    model_det.load_state_dict(torch.load(\"weights/detection_model.pth\"))\n",
    "    model_det.eval().to(\"cpu\")\n",
    "\n",
    "    model_cls = get_classification_model(num_labels=5)\n",
    "    model_cls.load_state_dict(torch.load(\"weights/classification_model.pth\"))\n",
    "    model_cls.eval().to(\"cpu\")\n",
    "\n",
    "    cls_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    results = []\n",
    "    for img_name in os.listdir(\"dataset_project_iapr2025/test\"):\n",
    "        if not img_name.endswith(('.jpg', '.png')): continue\n",
    "        image = Image.open(os.path.join(\"data/test_images\", img_name)).convert(\"RGB\")\n",
    "        image_tensor = F.to_tensor(image).unsqueeze(0)\n",
    "        outputs = model_det(image_tensor)\n",
    "        boxes = outputs[0]['boxes']\n",
    "\n",
    "        for i, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = box.int().tolist()\n",
    "            crop = image.crop((x1, y1, x2, y2))\n",
    "            crop = cls_transform(crop).unsqueeze(0)\n",
    "            preds = model_cls(crop).squeeze()\n",
    "\n",
    "            result = {\"filename\": img_name, \"box_id\": i}\n",
    "            for j in range(len(preds)):\n",
    "                result[f\"class_{j}\"] = preds[j].item()\n",
    "            results.append(result)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e221deab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n",
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)            # 打印 PyTorch 版本\n",
    "print(torch.version.cuda)           # 打印 CUDA 版本（None = 没有CUDA）\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8a1f342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gpu\n",
      "Epoch 1 done\n",
      "Epoch 2 done\n",
      "Epoch 3 done\n",
      "Epoch 4 done\n",
      "Epoch 5 done\n",
      "Epoch 6 done\n",
      "Epoch 7 done\n",
      "Epoch 8 done\n",
      "Epoch 9 done\n",
      "Epoch 10 done\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory weights does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m train_classification()\n\u001b[0;32m      3\u001b[0m run_pipeline()\n",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m, in \u001b[0;36mtrain_detection\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights/detection_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\iapr_project\\lib\\site-packages\\torch\\serialization.py:943\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[0;32m    940\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 943\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    944\u001b[0m         _save(\n\u001b[0;32m    945\u001b[0m             obj,\n\u001b[0;32m    946\u001b[0m             opened_zipfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    949\u001b[0m             _disable_byteorder_record,\n\u001b[0;32m    950\u001b[0m         )\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\iapr_project\\lib\\site-packages\\torch\\serialization.py:810\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\iapr_project\\lib\\site-packages\\torch\\serialization.py:781\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    778\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream, _compute_crc32)\n\u001b[0;32m    779\u001b[0m     )\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 781\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compute_crc32\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory weights does not exist."
     ]
    }
   ],
   "source": [
    "train_detection()\n",
    "train_classification()\n",
    "run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
